---
title: "Practical Machine Learning Assignment"
author: "Russ Macbeth"
date: "October 24, 2015"
output: html_document
---

## Summary

The goal is to predict if an individual is correctly preforming a barbell lift exercise or is preforming one of four incorrect methods, based on accelerometer reading from fitness trackers. To create the predictions we'll download the data, familiarize ourselves with the training data through data exploration, and create a model to predict the results of the 20 test cases. Included in the modeling we will apply cross validation and estimate the expected out of sample error.

## Getting the data

First we need to download and read in the data, which comes from http://groupware.les.inf.puc-rio.br/har 
```{r get_data, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv", method = "curl")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv", method = "curl")
training <- read.csv("training.csv")
testing <- read.csv("testing.csv")
```

```{r}
require(caret)
```
## Data Exploration

For reproducible results, we'll set the seed:
```{r}
set.seed(44321)
```

We are trying to predict which category of variable "classe" based on accelerometer data. Within the "classe" variable there are five options. The first, indicated as status "A", indicates the exercise was preformed correctly. Conversely, if the excercise was being preformed in one of four incorrect ways, the result is labeled "B" through "E".

Because we are trying to classify the result into one of the five classe categories, we will utilize the random forest method of machine learning instead of a regression model.

The data has `r length(testing)` variables. For this large amount of columns we cannot reasonabily expect to fully understand and analyze each column individually, particularly with such domain specific knowledge requires as gyrospcope coordinates. Instead, we will evaluate the dataset at a high level, removing variables that are detrimental to the model. 

From the plot below, we can see that the largest number of rows are represented by the correct exercise type with classfication A. 
```{r}
histogram(training$classe)
```

One area that can cause issues with applying any of our models is if there are a significant number of NA values. To see if this may need to be address we want a count of the NAs, which is `r sum(is.na(training))`. This warrants investigation for additional preprocessing. There are `r length(training$classe)` rows in our training set. We then discover there are `r sum(colSums(is.na(training))>(length(training$classe)/2))' columns with over 50% NA. We'll create a new dataset of only the columns with over 50% NA for further investigation. 

```{r}
highNA <- training[ , which(colSums(is.na(training))>(length(training$classe)/2))]
```

From that separate data we can see that the the number of complete rows without NAs is `r sum(complete.cases(highNA))`, which accounts for only `r 406/length(highNA[ , 1])` of the rows, and that the percent of NAs in the columns is `r mean(colSums(is.na(highNA))/length(highNA[ , 1]))`. To understand if all the rows have the same level on NAs we'll plot the percent of NAs per each column.

```{r}
barplot(colSums(is.na(highNA))/length(highNA[ , 1]))
colnumbers <- which(colnames(training) %in% colnames(highNA)) ## to get the NA columns to remove
```

Because of the consistent, extremely high number of NAs in the columns, applying any sort of standardizing to remove these NAs could easily prove misleading. Instead we will remove these columns from our modeling.

Another area of problem for the modeling is variables with significant blank values and variables with signficant "#DIV/0!" errors. We'll next identify these and remove them as well. 

There are `r sum(colSums(training[ , -colnumbers] == "") > (length(training$X)/2))` columns with over 50% blanks. When we look at the plot below, the bars indicate the count of blanks in each column. From that we can see that when there is a blank, there are over 14,000 of the 19,622 rows are blank. Because there are only `r 14411/19622` non blanks, we'll remove these columns as well since any standardizing would prove unrealiable. 

```{r}
barplot(colSums(training[ , -colnumbers] == ""))
```

We'll remove these columns, and add them to the columns to be removed for the high NA content with the code below.

```{r}
colnumbers <- which(colnames(training) %in% colnames(highNA)) ## to get the NA columns to remove
tmp <- training[ , -colnumbers]
blanks <- tmp[ , which(colSums(tmp == "") > 10000)]
colnumberblanks <- which(colnames(training) %in% colnames(blanks)) ## to get the blank columns to remove
colnumbersall <- c(colnumbers, colnumberblanks)
```

Now as we check for "#DIV/0" values we can see that all instances of "#DIV/0" were all in the same columns as the blanks, there are now `r sum(colSums(training[ , -colnumbersall] == "#DIV/0!"))` columns with "#DIV/0". At this point the data is ready to use.

After removing those variables with high blanks and N/As we can look at the rest of the data to see if there are others that logically will not be good for predictions. The first seven columns, including variables like subject name and timestamp, will not be meaningful for future predictions so we will remove those columns as well. We will create a new data set where we remove all of these columns and do the same to the testing data set.

```{r}
finaltrain <- training[ , -c(1:7, colnumbersall)]
finaltest <- testing[ , -c(1:7, colnumbersall)]
```

## Creating the model and setting up cross validation

To effectively test our model we need to separate part of the training data so we can cross validate against that data as we only run the final model once against the final test data. We'll name our data traintrain, for the training piece of the overall training set, and traintest, for the segment we will test against for cross validation. 

```{r}
inTrain <- createDataPartition(y = finaltrain$classe, p = 0.75, list = FALSE)
traintrain <- finaltrain[inTrain, ]
traintest <- finaltrain[-inTrain, ]
```

Now we'll create our first model using boosting with trees, method gmb. We are using trees because their is more than one level of classification we are trying to solve, making linear regression a poor choice.  

```{r Modelgbm, echo = TRUE, warning=FALSE, cache=TRUE, message=FALSE}
modfit <- train(classe ~ ., data = traintrain, method = "gbm", verbose = FALSE)
```

## Estimating Out of Sample Error through Cross Validation and Additional Models

To test our results we will predict based on our model against the traintrain set and then against the traintest data.

```{r}
predict1 <- predict(modfit, traintrain)
confusionMatrix(traintrain$classe, predict1)
```

From the confusion matrix we can see that we were 97.5% accurate against our training data. To cross validate we'll predict against our traintest data.

```{r}
predict1 <- predict(modfit, traintest)
confusionMatrix(traintest$classe, predict1)
```

Our accuracy has dropped to 96%. We have some slight overfitting. 

We will then apply this model to the predict our 20 test cases. 

```{r test_prediction, echo=TRUE, warning=FALSE, cache=FALSE, message=FALSE}
results <- predict(modfit, finaltest)
results
```

It's impossible to know if these results are accurate, however they look reasonable. 

A second model choice we will consider is using random forest without boosting.

```{r ModelRF, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
modfitRF <- train(classe ~ ., data = traintrain, method = "rf", verbose = FALSE)
```

To test our results we'll create confusion matrixes agaist both the traintrain and traintest sets. 

```{r}
confusionMatrix(traintrain$classe, predict(modfitRF, traintrain))
confusionMatrix(traintest$classe, predict(modfitRF, traintest))
```

The accuracy in this case is 100% against the traintrain data and 99% against the traintest data. 

We will again apply this model to the predict our 20 test cases. 

```{r test_predictionRF, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
resultsRF <- predict(modfitRF, finaltest)
resultsRF
```

Comparing the predictions for the finaltest set we get the same results from both models. We will use the second model, the random forest model without boosting, because it had higher accuracy against the traintest data. Because we received the same results from both of the models our expected accuracy on the testing data is between 96%-99% since that was the range of the accuracy from our cross validation. 