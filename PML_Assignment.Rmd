---
title: "Practical Machine Learning Assignment"
author: "Russ Macbeth"
date: "October 24, 2015"
output: html_document
---

## Summary

The goal is to predict if an individual is correctly preforming a barbell lift exercise or is preforming one of four incorrect methods, based on accelerometer reading from fitness trackers. To create the predictions we'll download the data, familiarize ourselves with the training data through data exploration, and create a model to predict the results of the 20 test cases. Included in the modeling we will apply cross validation and estimate the expected out of sample error.

## Getting the data

First we need to download and read in the data, which comes from http://groupware.les.inf.puc-rio.br/har 
```{r get_data, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv", method = "curl")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv", method = "curl")
training <- read.csv("training.csv")
testing <- read.csv("testing.csv")
```

```{r}
require(caret)
```
## Data Exploration

For reproducible results, we'll set the seed:
```{r}
set.seed(44321)
```

We are trying to predict which category of variable "classe" based on accelerometer data. Within the "classe" variable there are five options. The first, indicated as status "A", indicates the exercise was preformed correctly. Conversely, if the excercise was being preformed in one of four incorrect ways, the result is labeled "B" through "E".

Because we are trying to classify the result into one of the five classe options, we will utilize the random forest method of machine learning instead of a regression model.

The data has `r length(testing)` variables. For this large amount of columns we cannot reasonabily expect to fully understand and analyze each column individually, particularly with such domain specific knowledge requires as gyrospcope coordinates. Instead, we will evaluate the dataset at a high level, removing variables that are detrimental to the model. 

From the plot below, we can see that the largest number of rows are represented by the correct exercise type with classfication A. 
```{r}
histogram(training$classe)
```

One area that can cause issues with applying any of our models is if there are a significant number of NA values. To see if this may need to be address we want a count of the NAs, which is `r sum(is.na(training))`. This warrants investigation for additional preprocessing. There are `r length(training$classe)` rows in our training set. We then discover there are `r sum(colSums(is.na(training))>(length(training$classe)/2))' columns with over 50% NA. We'll create a new dataset of only the columns with over 50% NA for further investigation. 

```{r}
highNA <- training[ , which(colSums(is.na(training))>(length(training$classe)/2))]
```

From that separate data we can see that the the number of complete rows without NAs is `r sum(complete.cases(highNA))`, which accounts for only `r 406/length(highNA[ , 1])` of the rows, and that the percent of NAs in the columns is `r mean(colSums(is.na(highNA))/length(highNA[ , 1]))`. To understand if all the rows have the same level on NAs we'll plot the percent of NAs per each column.

```{r}
barplot(colSums(is.na(highNA))/length(highNA[ , 1]))
colnumbers <- which(colnames(training) %in% colnames(highNA)) ## to get the NA columns to remove
```

Because of the consistent, extremely high number of NAs in the columns, applying any sort of standardizing to remove these NAs could easily prove misleading. Instead we will remove these columns from our modeling.

Another area of problmes for the modeling is variables with significant blank values and variables with signficant "#DIV/0!" errors. We'll next identify these and remove them as well. 

There are `r sum(colSums(training[ , -colnumbers] == "") > (length(training$X)/2))` columns with over 50% blanks. When we look at the plot below, the bars indicate the count of blanks in each column. From that we can see that when there is a blank, there are over 14,000 of the 19,622 rows are blank. Because there are only `r 14411/19622` non blanks, we'll remove these columns as well since any standardizing would prove unrealiable. 

```{r}
barplot(colSums(training[ , -colnumbers] == ""))
```

We'll remove these columns, and add them to the columns to be removed for the high NA content with the code below.

```{r}
colnumbers <- which(colnames(training) %in% colnames(highNA)) ## to get the NA columns to remove
tmp <- training[ , -colnumbers]
blanks <- tmp[ , which(colSums(tmp == "") > 10000)]
colnumberblanks <- which(colnames(training) %in% colnames(blanks)) ## to get the blank columns to remove
colnumbersall <- c(colnumbers, colnumberblanks)
```

Now as we check for "#DIV/0" values we can see that all instances of "#DIV/0" were all in the same columns as the blanks, there are `r sum(colSums(training[ , -colnumbersall] == "#DIV/0!"))` columns with "#DIV/0". At this point the data is ready to use.

## Creating the model and setting up cross validation

To effectively test our model we need to separate part of the training data so we can test against that data as we only run the final model once against the final test data. We'll name our data traintrain, for the training piece of the overall training set, and traintest, for the segment we will test against for cross validation. 

```{r}
inTrain <- createDataPartition(y = training$classe, p = 0.75, list = FALSE)
traintrain <- training[inTrain, ]
traintest <- training[-inTrain, ]
```

Now we'll create a model using boosting with trees. We are using trees because their is more than one level of classification we are trying to solve, making linear regression a poor choice. We'll use the code: modfit <- train(classe ~ ., data = traintrain[ , -c(2:6, colnumbersall)], method = "gbm", verbose = "FALSE") and run the code with echo = FALSE to not display the text that comes with the model  

```{r Model, echo=FALSE, warning=FALSE, cache=TRUE, message=FALSE}

modfit <- train(classe ~ ., data = traintrain[ , -c(2:6, colnumbersall)], method = "gbm", verbose = "FALSE")
```


## Estimating Out of Sample Error through Cross Validation

To test our results we will predict based on our model against the traintrain set and then against the traintest data.

```{r}
predict1 <- predict(modfit, traintrain)
confusionMatrix(traintrain$classe, predict1)
```

From the confusion matrix we can see that we were 100% accurate against our training data, which looks good, though can make one nervous about having significantly overfitted to the sample data. To check for overfitting we'll predict against our traintest data for cross validation.

```{r}
predict1 <- predict(modfit, traintest)
confusionMatrix(traintest$classe, predict1)
```

From the confusion matrix results we can see we had 100% accuracy in our traintest data. Accordingly, we estimate that our out of sample error will be zero. 

We will then apply this model to the predict our 20 test cases. 

```{r test_prediction, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
results <- predict(modfit, testing)
results
```

Based on the results our predicted values are all A. This seems unlikely, and seems that there was significant overfitting. 

A second model choice we will consider is using random forest without boosting since boosting focuses heavilty on improved accuracy and may be causing overfitting.

```{r ModelRF, echo=FALSE, warning=FALSE, cache=TRUE, message=FALSE}
modfitRF <- train(classe ~ ., data = traintrain[ , -c(2:6, colnumbersall)], method = "rf", verbose = "FALSE")
```

```{r}
confusionMatrix(traintrain$classe, predict(modfitRF, traintrain))
confusionMatrix(traintest$classe, predict(modfitRF, traintest))
```


We will again apply this model to the predict our 20 test cases. 

```{r test_predictionRF, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
resultsRF <- predict(modfitRF, testing)
resultsRF
```
